{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6b228c-e5e8-46fd-b5fb-7ea243408afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pygrinder import mcar, calc_missing_rate\n",
    "from benchpots.datasets import preprocess_physionet2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e49dc363-1294-41d5-aa05-d6abc208cd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:51:36 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2025-06-19 22:51:36 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2025-06-19 22:51:36 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2025-06-19 22:51:36 [INFO]: Loaded successfully!\n",
      "2025-06-19 22:51:38 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2025-06-19 22:51:38 [INFO]: 22900 values masked out in the val set as ground truth, take 10.04% of the original observed values\n",
      "2025-06-19 22:51:38 [INFO]: 28501 values masked out in the test set as ground truth, take 9.90% of the original observed values\n",
      "2025-06-19 22:51:38 [INFO]: Total sample number: 3997\n",
      "2025-06-19 22:51:38 [INFO]: Training set size: 2557 (63.97%)\n",
      "2025-06-19 22:51:38 [INFO]: Validation set size: 640 (16.01%)\n",
      "2025-06-19 22:51:38 [INFO]: Test set size: 800 (20.02%)\n",
      "2025-06-19 22:51:38 [INFO]: Number of steps: 48\n",
      "2025-06-19 22:51:38 [INFO]: Number of features: 37\n",
      "2025-06-19 22:51:38 [INFO]: Train set missing rate: 79.67%\n",
      "2025-06-19 22:51:38 [INFO]: Validating set missing rate: 81.94%\n",
      "2025-06-19 22:51:38 [INFO]: Test set missing rate: 81.74%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2557, 48, 37)\n",
      "(640, 48, 37)\n",
      "We have 79.7% values missing in train_X\n"
     ]
    }
   ],
   "source": [
    "data = preprocess_physionet2012(subset='set-a',rate=0.1) # Our ecosystem libs will automatically download and extract it\n",
    "train_X, val_X, test_X = data[\"train_X\"], data[\"val_X\"], data[\"test_X\"]\n",
    "print(train_X.shape)  # (n_samples, n_steps, n_features)\n",
    "print(val_X.shape)  # samples (n_samples) in train set and val set are different, but they have the same sequence len (n_steps) and feature dim (n_features)\n",
    "print(f\"We have {calc_missing_rate(train_X):.1%} values missing in train_X\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb35d76-4617-4723-91ba-f08d8b74ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = {\"X\": train_X}  # in training set, simply put the incomplete time series into it\n",
    "val_set = {\n",
    "    \"X\": val_X,\n",
    "    \"X_ori\": data[\"val_X_ori\"],  # in validation set, we need ground truth for evaluation and picking the best model checkpoint\n",
    "}\n",
    "test_set = {\"X\": test_X}  # in test set, only give the testing incomplete time series for model to impute\n",
    "test_X_ori = data[\"test_X_ori\"]  # test_X_ori bears ground truth for evaluation\n",
    "indicating_mask = np.isnan(test_X) ^ np.isnan(test_X_ori)  # mask indicates the values that are missing in X but not in X_ori, i.e. where the gt values are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a1de1f6-f93f-496b-ae94-d42f7661d59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:52:11 [INFO]: No given device, using default device: cpu\n",
      "2025-06-19 22:52:11 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-19 22:52:11 [INFO]: Using customized MAE as the training loss function.\n",
      "2025-06-19 22:52:11 [INFO]: Using customized MSE as the validation metric function.\n",
      "2025-06-19 22:52:11 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,378,358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗\n",
      "╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║\n",
      "   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║\n",
      "   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║\n",
      "   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║\n",
      "   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝\n",
      "ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 22:52:23 [WARNING]: ‼️ Training got interrupted by the user. Exist now ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Something is wrong. best_loss is NaN/Inf after training.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpypots\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calc_mae\n\u001b[1;32m      3\u001b[0m saits \u001b[38;5;241m=\u001b[39m SAITS(n_steps\u001b[38;5;241m=\u001b[39mtrain_X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], n_features\u001b[38;5;241m=\u001b[39mtrain_X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], n_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, n_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, d_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, d_v\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, d_ffn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m saits\u001b[38;5;241m.\u001b[39mfit(train_set, val_set)  \u001b[38;5;66;03m# train the model on the dataset\u001b[39;00m\n\u001b[1;32m      5\u001b[0m imputation \u001b[38;5;241m=\u001b[39m saits\u001b[38;5;241m.\u001b[39mimpute(test_set)  \u001b[38;5;66;03m# impute the originally-missing values and artificially-missing values\u001b[39;00m\n\u001b[1;32m      6\u001b[0m mae \u001b[38;5;241m=\u001b[39m calc_mae(imputation, np\u001b[38;5;241m.\u001b[39mnan_to_num(test_X_ori), indicating_mask)  \u001b[38;5;66;03m# calculate mean absolute error on the ground truth (artificially-missing values)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages/pypots/imputation/saits/model.py:275\u001b[0m, in \u001b[0;36mSAITS.fit\u001b[0;34m(self, train_set, val_set, file_type)\u001b[0m\n\u001b[1;32m    267\u001b[0m     val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    268\u001b[0m         val_dataset,\n\u001b[1;32m    269\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m    270\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    271\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[1;32m    272\u001b[0m     )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Step 2: train the model and freeze it\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_model(train_dataloader, val_dataloader)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model_dict)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# Step 3: save the model if necessary\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages/pypots/base.py:825\u001b[0m, in \u001b[0;36mBaseNNModel._train_model\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m(\n\u001b[1;32m    819\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining got interrupted. Please investigate the error printed above.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    820\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel got trained and will load the best checkpoint so far for testing.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt want it, please try fit() again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    822\u001b[0m         )\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_loss) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_loss\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomething is wrong. best_loss is NaN/Inf after training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    827\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training. The best model is from epoch#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Something is wrong. best_loss is NaN/Inf after training."
     ]
    }
   ],
   "source": [
    "from pypots.imputation import SAITS  # import the model you want to use\n",
    "from pypots.nn.functional import calc_mae\n",
    "saits = SAITS(n_steps=train_X.shape[1], n_features=train_X.shape[2], n_layers=2, d_model=256, n_heads=4, d_k=64, d_v=64, d_ffn=128, dropout=0.1, epochs=5)\n",
    "saits.fit(train_set, val_set)  # train the model on the dataset\n",
    "imputation = saits.impute(test_set)  # impute the originally-missing values and artificially-missing values\n",
    "mae = calc_mae(imputation, np.nan_to_num(test_X_ori), indicating_mask)  # calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "saits.save(\"save_it_here/saits_physionet2012.pypots\")  # save the model for future use\n",
    "saits.load(\"save_it_here/saits_physionet2012.pypots\")  # reload the serialized model file for following imputation or training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d68ca-16a8-46bb-9737-64ff708448c7",
   "metadata": {},
   "source": [
    "# Training SAITS for VitalDb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab9171c-5ea4-4c26-af14-555b183f2100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vitaldb in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from vitaldb) (2.1.3)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from vitaldb) (2.2.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from vitaldb) (2.32.3)\n",
      "Requirement already satisfied: wfdb in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from vitaldb) (4.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from pandas->vitaldb) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from pandas->vitaldb) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from pandas->vitaldb) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from requests->vitaldb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from requests->vitaldb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from requests->vitaldb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from requests->vitaldb) (2025.1.31)\n",
      "Requirement already satisfied: aiohttp>=3.10.11 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from wfdb->vitaldb) (3.11.10)\n",
      "Requirement already satisfied: fsspec>=2023.10.0 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from wfdb->vitaldb) (2024.12.0)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from wfdb->vitaldb) (3.10.0)\n",
      "Requirement already satisfied: scipy>=1.13.0 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from wfdb->vitaldb) (1.15.2)\n",
      "Requirement already satisfied: soundfile>=0.10.0 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from wfdb->vitaldb) (0.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (1.18.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->vitaldb) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from soundfile>=0.10.0->wfdb->vitaldb) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb->vitaldb) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install vitaldb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc76b68-fabe-43fd-a0ce-9f5ebd69bf25",
   "metadata": {},
   "source": [
    "## Load & resample all VitalDB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "919ae154-224b-4159-9f6b-9b26623fda51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SNUADC/ART ', 'SNUADC/ECG_II', 'SNUADC/ECG_V5 ', 'SNUADC/PLETH', 'Primus/CO2', 'BIS/EEG1_WAV', 'BIS/EEG2_WAV']\n",
      "Dataset shape: (1, 5770575, 7)   (skipped 0 files)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, vitaldb as vdb, numpy as np, pandas as pd\n",
    "\n",
    "# ---------- settings ----------\n",
    "vital_dir      = Path(\"/Users/muhammadaneequz.zaman/Dropbox/Digital Twin (Umer Huzaifa)/vitalDB_v1\")\n",
    "track_keep     = [\"SNUADC/ART \", \"SNUADC/ECG_II\", \"SNUADC/ECG_V5 \", \n",
    "                  \"SNUADC/PLETH\", \"Primus/CO2\", \"BIS/EEG1_WAV\", \"BIS/EEG2_WAV\"]   # pick tracks present in *every* case\n",
    "target_fs      = \"1S\"                                           # 1-second grid\n",
    "sequence_len   = 60*10                                          # 10-min snippets → n_steps = 600\n",
    "# ------------------------------\n",
    "\n",
    "def read_one(file_path, tracks_keep):\n",
    "    \"\"\"\n",
    "    Return a pandas DataFrame whose index is the native VitalDB timestamp\n",
    "    (datetime) and whose columns are the requested track names.\n",
    "    Missing samples remain as NaN.\n",
    "    \"\"\"\n",
    "    all_tracks = vdb.vital_trks(str(file_path))\n",
    "    #print(all_tracks)\n",
    "    print(tracks_keep)\n",
    "    #numeric_tracks = [t for t in all_tracks if t in tracks_keep]\n",
    "\n",
    "    # if not numeric_tracks:\n",
    "    #     raise ValueError(\"none of the requested tracks in this file\")\n",
    "    \n",
    "    return vdb.vital_recs(\n",
    "        str(file_path),\n",
    "        # track_names=all_tracks,\n",
    "        track_names=tracks_keep,\n",
    "        return_timestamp=False,      # keep absolute clock time\n",
    "        return_datetime=False,\n",
    "        return_pandas=True,\n",
    "    )\n",
    "\n",
    "all_cases = []                      # dict: filename  -> DataFrame\n",
    "bad_files = []\n",
    "for f in sorted(vital_dir.glob(\"*.vital\")):\n",
    "    try:\n",
    "        df = read_one(f, track_keep)\n",
    "        df_numeric = df.apply(pd.to_numeric, errors=\"coerce\")  # strings -> NaN\n",
    "        all_cases.append(df_numeric.to_numpy(dtype=np.float32))   # shape (sequence_len, n_features)\n",
    "    except Exception as e:\n",
    "        print(f\"skip {f.name}: {e}\")\n",
    "        bad_files.append(f.name)\n",
    "\n",
    "dataset = np.stack(all_cases, axis=0)               # ==> (n_samples, n_steps, n_features)\n",
    "print(\"Dataset shape:\", dataset.shape, \"  (skipped\", len(bad_files), \"files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752c944b-f2cb-47b5-bb78-dbc740efcb2e",
   "metadata": {},
   "source": [
    "## Train / val / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef06c2b1-5c1f-45ae-a90a-f32f9bbc182a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segments shape: (9617, 600, 7)\n",
      "train (6947, 600, 7) val (1227, 600, 7) test (1443, 600, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X            = dataset.squeeze(0)          # → (5_771_049, 80)\n",
    "window       = 600                         # 10 minutes if you resample to 1 Hz\n",
    "stride       = 600                         # non-overlapping; use <window for overlap\n",
    "segments = [\n",
    "    X[i : i + window]\n",
    "    for i in range(0, X.shape[0] - window + 1, stride)\n",
    "]\n",
    "segments = np.stack(segments)              # (n_segments, 600, 80)\n",
    "print(\"segments shape:\", segments.shape)\n",
    "\n",
    "train_X, test_X = train_test_split(segments,  test_size=0.15, random_state=42)\n",
    "train_X,  val_X = train_test_split(train_X, test_size=0.15, random_state=42)\n",
    "\n",
    "print(\"train\", train_X.shape, \"val\", val_X.shape, \"test\", test_X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e2f17-1aa0-43e6-8a14-b430cf954e07",
   "metadata": {},
   "source": [
    "## Add extra synthetic missingness on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21006630-50cd-4582-b7b6-e391dd4564e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real miss rate train  : 28.6%\n",
      "Real+fake miss rate val: 35.7%\n"
     ]
    }
   ],
   "source": [
    "from pygrinder import mcar, calc_missing_rate\n",
    "\n",
    "val_X_ori = val_X.copy()             # keep a pristine copy\n",
    "val_X     = mcar(val_X, p=0.10)   # mask-at-random 10 %\n",
    "\n",
    "test_X_ori = test_X.copy()           # ditto for the test set\n",
    "indicating_mask = np.isnan(test_X) ^ np.isnan(test_X_ori)\n",
    "print(f\"Real miss rate train  : {calc_missing_rate(train_X):.1%}\")\n",
    "print(f\"Real+fake miss rate val: {calc_missing_rate(val_X):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1e994-b919-427c-bb7f-d8270ee0d63c",
   "metadata": {},
   "source": [
    "## Wrap in the dictionaries SAITS expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "722a1009-f705-4b7b-89ba-5811dc70980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = {\"X\": train_X}\n",
    "val_set   = {\"X\": val_X, \"X_ori\": val_X_ori}\n",
    "test_set  = {\"X\": test_X}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686547e7-9007-4dfe-92a0-c7ab419fcbee",
   "metadata": {},
   "source": [
    "## Instantiate, train, evaluate just like the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07e674a1-c15d-4f8e-91db-062a8a087493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 12:54:51 [INFO]: Using the given device: cpu\n",
      "2025-06-21 12:54:51 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-21 12:54:51 [INFO]: Using customized MAE as the training loss function.\n",
      "2025-06-21 12:54:51 [INFO]: Using customized MSE as the validation metric function.\n",
      "2025-06-21 12:54:51 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,331,942\n"
     ]
    }
   ],
   "source": [
    "from pypots.imputation import SAITS\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "saits = SAITS(\n",
    "    n_steps   = train_X.shape[1],\n",
    "    n_features= train_X.shape[2],\n",
    "    n_layers  = 2,\n",
    "    d_model   = 256,\n",
    "    n_heads   = 4,\n",
    "    d_k       = 64,\n",
    "    d_v       = 64,\n",
    "    d_ffn     = 128,\n",
    "    dropout   = 0.1,\n",
    "    epochs    = 20,\n",
    "    patience  = 5,                  # early-stop patience (optional)\n",
    "    device    = \"cpu\"\n",
    "    #device    = \"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d74011-184c-409a-8d54-28748477ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "saits.fit(train_set, val_set)\n",
    "\n",
    "# ---- test-time imputation ----\n",
    "imputation = saits.impute(test_set)               # same shape as test_X\n",
    "mae        = calc_mae(imputation, np.nan_to_num(test_X_ori), indicating_mask)\n",
    "print(\"MAE on held-out values:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a6d41ae-8fe7-40e4-96dd-599bf0950e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 13:20:09 [WARNING]: ‼️ File models/saits_vitaldb.pypots exists. Argument `overwrite` is True. Overwriting now...\n",
      "2025-06-22 13:20:09 [INFO]: Saved the model to models/saits_vitaldb.pypots\n"
     ]
    }
   ],
   "source": [
    "saits.save(\"models/saits_vitaldb.pypots\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5acace80-b38d-4adf-821b-77d9a882e7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITATION.cff                  \u001b[34mmodels\u001b[m\u001b[m\n",
      "conda_env_dependencies.yml    \u001b[34mNNI_tuning\u001b[m\u001b[m\n",
      "\u001b[34mconfigs\u001b[m\u001b[m                       Paper_SAITS.pdf\n",
      "\u001b[34mdataset_generating_scripts\u001b[m\u001b[m    README.md\n",
      "\u001b[34mfigs\u001b[m\u001b[m                          run_models.py\n",
      "Global_Config.py              \u001b[34msave_it_here\u001b[m\u001b[m\n",
      "LICENSE                       Simple_example.ipynb\n",
      "\u001b[34mmodeling\u001b[m\u001b[m                      Simple_RNN_on_imputed_data.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94f68cba-1a74-4f09-9e18-ceca7cf7c95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/digitalTwin/lib/python3.12/site-packages/pypots/base.py:384: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_file = torch.load(path, map_location=map_location)\n",
      "2025-06-20 11:49:36 [INFO]: Model loaded successfully from models/saits_vitaldb.pypots\n"
     ]
    }
   ],
   "source": [
    "saits.load(\"models/saits_vitaldb.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c2f994-a814-4df4-9e84-216771efd0fd",
   "metadata": {},
   "source": [
    "## IMPUTE THE ORIGINAL DATA  (train + val + test, NaNs only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe241f8-8d77-4990-9c50-3bd69f49572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all three splits so we fill every real gap in one go\n",
    "orig_concat = np.concatenate([train_X, val_X_ori, test_X_ori], axis=0)\n",
    "orig_imputed = saits.impute({\"X\": orig_concat})        # <-- returns np.ndarray\n",
    "\n",
    "# You can now split it back if you want\n",
    "n_train = train_X.shape[0]\n",
    "n_val   = val_X_ori.shape[0]\n",
    "imputed_train = orig_imputed[:n_train]\n",
    "imputed_val_full = orig_imputed[n_train:n_train+n_val]  # val set with real NaNs filled\n",
    "imputed_test  = orig_imputed[n_train+n_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023756a-1c55-4d41-bb10-423165aec332",
   "metadata": {},
   "source": [
    "## IMPUTE THE SYNTHETICALLY MASKED VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a69221e-2445-4877-ae48-c9f033034123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on synthetically missing points in val set: 4.696758137825921\n"
     ]
    }
   ],
   "source": [
    "# val_X has *extra* 10 % MCAR holes; we already built val_set = {\"X\": val_X}\n",
    "imputed_val_masked = saits.impute(val_set)              # same shape as val_X\n",
    "# evaluate MAE on those artificial holes\n",
    "masked_mae = calc_mae(imputed_val_masked, \n",
    "                      np.nan_to_num(val_X_ori), \n",
    "                      np.isnan(val_X) ^ np.isnan(val_X_ori))\n",
    "print(\"MAE on synthetically missing points in val set:\", masked_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064df81-99bf-4e98-8c2f-0bb068954999",
   "metadata": {},
   "source": [
    "## SHOW 15 RANDOM IMPUTATIONS vs. GROUND-TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1d80f5f-d006-4374-90d6-2c86dbdb93cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   0,    0,    0, ..., 1226, 1226, 1226]), array([  2,   5,   6, ..., 592, 594, 596]), array([4, 6, 3, ..., 5, 1, 1]))\n",
      "\n",
      "===  SAITS imputation on synthetic holes (random 30)  ===\n",
      " sample#  time_step       channel  ground_truth  imputed  abs_error\n",
      "     138        587  BIS/EEG1_WAV        42.250   35.298   6.952000\n",
      "    1091        517  SNUADC/PLETH        29.260   30.146   0.886000\n",
      "     447        549    Primus/CO2        22.275   23.010   0.735000\n",
      "     276         98  BIS/EEG2_WAV        13.250    9.953   3.297000\n",
      "     370        592 SNUADC/ECG_II        -0.049    0.000   0.049000\n",
      "     374        449 SNUADC/ECG_II         0.327    0.016   0.311000\n",
      "     541        308  BIS/EEG2_WAV        11.700   29.209  17.509001\n",
      "     147        174  SNUADC/PLETH        35.184   35.505   0.320000\n",
      "     803        438  BIS/EEG2_WAV        23.400   23.049   0.351000\n",
      "      43        469  BIS/EEG1_WAV        48.500   53.088   4.588000\n",
      "     975        438 SNUADC/ECG_II         0.781    0.014   0.767000\n",
      "    1223        469  BIS/EEG1_WAV        33.150   28.303   4.847000\n",
      "     287         41  BIS/EEG1_WAV        19.450   15.130   4.320000\n",
      "     366        533  BIS/EEG2_WAV        33.050   31.793   1.257000\n",
      "    1114        361  BIS/EEG1_WAV        11.300    9.299   2.001000\n",
      "     760        471 SNUADC/ECG_II         0.011    0.012   0.002000\n",
      "      37        425  BIS/EEG2_WAV         8.900    9.263   0.363000\n",
      "     215        525  BIS/EEG2_WAV        19.150   18.506   0.644000\n",
      "     263        101  BIS/EEG2_WAV        20.150   18.807   1.343000\n",
      "     307        240  BIS/EEG1_WAV         7.650   13.805   6.155000\n",
      "    1078        265 SNUADC/ECG_II        -0.414    0.021   0.435000\n",
      "     497        292    Primus/CO2         1.050    0.354   0.696000\n",
      "     410        429  SNUADC/PLETH         0.426    1.328   0.902000\n",
      "    1151        223  BIS/EEG2_WAV        27.000   25.213   1.787000\n",
      "     617        475  BIS/EEG2_WAV         2.450    4.775   2.325000\n",
      "     350        168  BIS/EEG2_WAV        18.450   17.885   0.565000\n",
      "     700        589  BIS/EEG1_WAV        23.250   23.399   0.149000\n",
      "     875        591  BIS/EEG2_WAV        30.350   29.814   0.536000\n",
      "     623        288  BIS/EEG1_WAV        26.200   21.972   4.228000\n",
      "     512        324  BIS/EEG2_WAV        20.250   20.564   0.314000\n"
     ]
    }
   ],
   "source": [
    "import random, pandas as pd\n",
    "\n",
    "feature_names = track_keep                        # your 7 channels in that order\n",
    "mask_idx = np.where((np.isnan(val_X)) & ~np.isnan(val_X_ori))  # positions you hid\n",
    "n_show = min(30, mask_idx[0].size)               # show up to 15 rows\n",
    "rows = random.sample(range(mask_idx[0].size), n_show)\n",
    "\n",
    "print(mask_idx)\n",
    "records = []\n",
    "for k in rows:\n",
    "    s, t, f = mask_idx[0][k], mask_idx[1][k], mask_idx[2][k]\n",
    "    records.append({\n",
    "        \"sample#\":    s,\n",
    "        \"time_step\":  t,\n",
    "        \"channel\":    feature_names[f],\n",
    "        \"ground_truth\": float(val_X_ori[s, t, f]),\n",
    "        \"imputed\":     float(imputed_val_masked[s, t, f]),\n",
    "        \"abs_error\":   abs(val_X_ori[s, t, f] - imputed_val_masked[s, t, f]),\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(records)\n",
    "print(\"\\n===  SAITS imputation on synthetic holes (random 30)  ===\")\n",
    "print(comparison_df.round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08ec67-1255-435d-a1f7-7ed8ad7d5bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
